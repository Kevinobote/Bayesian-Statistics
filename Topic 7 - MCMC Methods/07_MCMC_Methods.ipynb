{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic 7: Stochastic Simulation and Markov Chain Monte Carlo (MCMC)\n",
    "\n",
    "## Learning Objectives\n",
    "- Understand the need for MCMC in Bayesian computation\n",
    "- Master different MCMC algorithms (Metropolis, Gibbs, HMC)\n",
    "- Diagnose convergence and assess chain quality\n",
    "- Apply MCMC to complex models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as stats\n",
    "import pymc as pm\n",
    "import arviz as az\n",
    "from scipy.stats import multivariate_normal\n",
    "\n",
    "plt.style.use('seaborn-v0_8')\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Why MCMC?\n",
    "\n",
    "### The Challenge:\n",
    "Most Bayesian posteriors are intractable:\n",
    "$$p(\\theta|D) = \\frac{p(D|\\theta)p(\\theta)}{\\int p(D|\\theta)p(\\theta)d\\theta}$$\n",
    "\n",
    "The denominator (evidence) is often impossible to compute analytically.\n",
    "\n",
    "### MCMC Solution:\n",
    "Generate samples $\\theta^{(1)}, \\theta^{(2)}, \\ldots, \\theta^{(T)}$ from $p(\\theta|D)$ without computing the normalizing constant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Metropolis Algorithm\n",
    "\n",
    "### Algorithm:\n",
    "1. Start with initial value $\\theta^{(0)}$\n",
    "2. For $t = 1, 2, \\ldots, T$:\n",
    "   - Propose: $\\theta^* \\sim q(\\theta^*|\\theta^{(t-1)})$\n",
    "   - Accept with probability: $\\alpha = \\min\\left(1, \\frac{p(\\theta^*|D)}{p(\\theta^{(t-1)}|D)} \\cdot \\frac{q(\\theta^{(t-1)}|\\theta^*)}{q(\\theta^*|\\theta^{(t-1)})}\\right)$\n",
    "   - If accepted: $\\theta^{(t)} = \\theta^*$, else: $\\theta^{(t)} = \\theta^{(t-1)}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metropolis_sampler(log_posterior, initial_value, n_samples, proposal_sd):\n",
    "    \"\"\"\n",
    "    Simple Metropolis sampler for univariate parameter\n",
    "    \"\"\"\n",
    "    samples = np.zeros(n_samples)\n",
    "    current = initial_value\n",
    "    n_accepted = 0\n",
    "    \n",
    "    for i in range(n_samples):\n",
    "        # Propose new state\n",
    "        proposal = current + np.random.normal(0, proposal_sd)\n",
    "        \n",
    "        # Calculate acceptance probability\n",
    "        log_alpha = log_posterior(proposal) - log_posterior(current)\n",
    "        alpha = min(1, np.exp(log_alpha))\n",
    "        \n",
    "        # Accept or reject\n",
    "        if np.random.random() < alpha:\n",
    "            current = proposal\n",
    "            n_accepted += 1\n",
    "        \n",
    "        samples[i] = current\n",
    "    \n",
    "    acceptance_rate = n_accepted / n_samples\n",
    "    return samples, acceptance_rate\n",
    "\n",
    "# Example: Sample from Beta(3, 2) using Metropolis\n",
    "def log_beta_posterior(theta):\n",
    "    \"\"\"Log density of Beta(3, 2)\"\"\"\n",
    "    if 0 <= theta <= 1:\n",
    "        return stats.beta.logpdf(theta, 3, 2)\n",
    "    else:\n",
    "        return -np.inf\n",
    "\n",
    "# Run Metropolis with different proposal standard deviations\n",
    "n_samples = 5000\n",
    "proposal_sds = [0.1, 0.5, 1.0]\n",
    "true_dist = stats.beta(3, 2)\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 8))\n",
    "\n",
    "for i, prop_sd in enumerate(proposal_sds):\n",
    "    samples, acc_rate = metropolis_sampler(log_beta_posterior, 0.5, n_samples, prop_sd)\n",
    "    \n",
    "    # Trace plot\n",
    "    axes[0, i].plot(samples[:1000], alpha=0.7)\n",
    "    axes[0, i].set_title(f'Trace (σ={prop_sd}, acc={acc_rate:.2f})')\n",
    "    axes[0, i].set_ylabel('θ')\n",
    "    axes[0, i].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Histogram vs true density\n",
    "    axes[1, i].hist(samples[1000:], bins=50, density=True, alpha=0.7, \n",
    "                   label='MCMC samples')\n",
    "    \n",
    "    x = np.linspace(0, 1, 100)\n",
    "    axes[1, i].plot(x, true_dist.pdf(x), 'r-', linewidth=2, label='True density')\n",
    "    axes[1, i].set_title(f'Posterior (σ={prop_sd})')\n",
    "    axes[1, i].set_xlabel('θ')\n",
    "    axes[1, i].set_ylabel('Density')\n",
    "    axes[1, i].legend()\n",
    "    axes[1, i].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Metropolis Algorithm Results:\")\n",
    "print(\"Proposal SD\\tAcceptance Rate\\tEffective Sample Size\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "for prop_sd in proposal_sds:\n",
    "    samples, acc_rate = metropolis_sampler(log_beta_posterior, 0.5, n_samples, prop_sd)\n",
    "    \n",
    "    # Calculate effective sample size (rough approximation)\n",
    "    autocorr = np.correlate(samples - np.mean(samples), \n",
    "                           samples - np.mean(samples), mode='full')\n",
    "    autocorr = autocorr[autocorr.size // 2:]\n",
    "    autocorr = autocorr / autocorr[0]\n",
    "    \n",
    "    # Find first negative autocorrelation\n",
    "    first_negative = np.where(autocorr < 0)[0]\n",
    "    if len(first_negative) > 0:\n",
    "        tau = first_negative[0]\n",
    "    else:\n",
    "        tau = len(autocorr)\n",
    "    \n",
    "    ess = n_samples / (2 * tau + 1) if tau > 0 else n_samples\n",
    "    \n",
    "    print(f\"{prop_sd}\\t\\t{acc_rate:.3f}\\t\\t\\t{ess:.0f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Gibbs Sampling\n",
    "\n",
    "### When to Use:\n",
    "When conditional distributions are available in closed form.\n",
    "\n",
    "### Algorithm:\n",
    "For parameters $(\\theta_1, \\theta_2)$:\n",
    "1. Sample $\\theta_1^{(t)} \\sim p(\\theta_1|\\theta_2^{(t-1)}, D)$\n",
    "2. Sample $\\theta_2^{(t)} \\sim p(\\theta_2|\\theta_1^{(t)}, D)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gibbs sampling example: Bivariate normal with missing data\n",
    "# Generate data with missing values\n",
    "np.random.seed(42)\n",
    "n = 50\n",
    "true_mu = np.array([2, 3])\n",
    "true_sigma = np.array([[1, 0.7], [0.7, 1]])\n",
    "\n",
    "# Generate complete data\n",
    "data_complete = multivariate_normal.rvs(true_mu, true_sigma, n)\n",
    "\n",
    "# Introduce missing values in second variable\n",
    "missing_mask = np.random.random(n) < 0.3\n",
    "data_observed = data_complete.copy()\n",
    "data_observed[missing_mask, 1] = np.nan\n",
    "\n",
    "print(f\"Data: {n} observations, {np.sum(missing_mask)} missing in variable 2\")\n",
    "\n",
    "def gibbs_bivariate_normal(data, n_samples, burnin=1000):\n",
    "    \"\"\"\n",
    "    Gibbs sampler for bivariate normal with missing data\n",
    "    \"\"\"\n",
    "    n_obs = len(data)\n",
    "    \n",
    "    # Initialize missing values\n",
    "    data_imputed = data.copy()\n",
    "    missing_idx = np.isnan(data[:, 1])\n",
    "    data_imputed[missing_idx, 1] = np.random.normal(0, 1, np.sum(missing_idx))\n",
    "    \n",
    "    # Storage\n",
    "    samples_mu = np.zeros((n_samples, 2))\n",
    "    samples_sigma = np.zeros((n_samples, 2, 2))\n",
    "    samples_missing = np.zeros((n_samples, np.sum(missing_idx)))\n",
    "    \n",
    "    # Priors (conjugate)\n",
    "    mu_0 = np.array([0, 0])\n",
    "    lambda_0 = np.eye(2) * 0.1\n",
    "    nu_0 = 3\n",
    "    psi_0 = np.eye(2)\n",
    "    \n",
    "    for i in range(n_samples + burnin):\n",
    "        # Sample missing values\n",
    "        if np.any(missing_idx):\n",
    "            for j in np.where(missing_idx)[0]:\n",
    "                # Conditional distribution for missing y2 given y1\n",
    "                y1 = data_imputed[j, 0]\n",
    "                \n",
    "                # Current estimates\n",
    "                if i == 0:\n",
    "                    mu_current = np.array([0, 0])\n",
    "                    sigma_current = np.eye(2)\n",
    "                else:\n",
    "                    mu_current = samples_mu[max(0, i-1)]\n",
    "                    sigma_current = samples_sigma[max(0, i-1)]\n",
    "                \n",
    "                # Conditional mean and variance\n",
    "                mu_cond = mu_current[1] + sigma_current[1,0]/sigma_current[0,0] * (y1 - mu_current[0])\n",
    "                var_cond = sigma_current[1,1] - sigma_current[1,0]**2/sigma_current[0,0]\n",
    "                \n",
    "                data_imputed[j, 1] = np.random.normal(mu_cond, np.sqrt(var_cond))\n",
    "        \n",
    "        # Sample mu (conjugate normal)\n",
    "        data_mean = np.mean(data_imputed, axis=0)\n",
    "        lambda_n = lambda_0 + n_obs * np.eye(2)\n",
    "        mu_n = np.linalg.solve(lambda_n, lambda_0 @ mu_0 + n_obs * data_mean)\n",
    "        \n",
    "        if i == 0:\n",
    "            sigma_current = np.eye(2)\n",
    "        \n",
    "        mu_sample = multivariate_normal.rvs(mu_n, np.linalg.inv(lambda_n))\n",
    "        \n",
    "        # Sample sigma (conjugate inverse-Wishart)\n",
    "        centered_data = data_imputed - mu_sample\n",
    "        psi_n = psi_0 + centered_data.T @ centered_data\n",
    "        nu_n = nu_0 + n_obs\n",
    "        \n",
    "        # Sample from inverse-Wishart (simplified)\n",
    "        sigma_sample = stats.invwishart.rvs(nu_n, psi_n)\n",
    "        \n",
    "        # Store samples (after burnin)\n",
    "        if i >= burnin:\n",
    "            idx = i - burnin\n",
    "            samples_mu[idx] = mu_sample\n",
    "            samples_sigma[idx] = sigma_sample\n",
    "            if np.any(missing_idx):\n",
    "                samples_missing[idx] = data_imputed[missing_idx, 1]\n",
    "    \n",
    "    return samples_mu, samples_sigma, samples_missing\n",
    "\n",
    "# Run Gibbs sampler\n",
    "n_samples = 2000\n",
    "mu_samples, sigma_samples, missing_samples = gibbs_bivariate_normal(data_observed, n_samples)\n",
    "\n",
    "print(\"\\nGibbs Sampling Results:\")\n",
    "print(f\"True μ: {true_mu}\")\n",
    "print(f\"Estimated μ: {np.mean(mu_samples, axis=0)}\")\n",
    "print(f\"\\nTrue Σ:\")\n",
    "print(true_sigma)\n",
    "print(f\"Estimated Σ:\")\n",
    "print(np.mean(sigma_samples, axis=0))\n",
    "\n",
    "# Visualization\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 8))\n",
    "\n",
    "# Trace plots for mu\n",
    "axes[0, 0].plot(mu_samples[:, 0], alpha=0.7, label='μ₁')\n",
    "axes[0, 0].plot(mu_samples[:, 1], alpha=0.7, label='μ₂')\n",
    "axes[0, 0].axhline(true_mu[0], color='blue', linestyle='--', alpha=0.7)\n",
    "axes[0, 0].axhline(true_mu[1], color='orange', linestyle='--', alpha=0.7)\n",
    "axes[0, 0].set_title('Trace: Mean Parameters')\n",
    "axes[0, 0].set_ylabel('Value')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Trace plots for sigma\n",
    "axes[0, 1].plot(sigma_samples[:, 0, 0], alpha=0.7, label='σ₁₁')\n",
    "axes[0, 1].plot(sigma_samples[:, 1, 1], alpha=0.7, label='σ₂₂')\n",
    "axes[0, 1].plot(sigma_samples[:, 0, 1], alpha=0.7, label='σ₁₂')\n",
    "axes[0, 1].axhline(true_sigma[0, 0], color='blue', linestyle='--', alpha=0.7)\n",
    "axes[0, 1].axhline(true_sigma[1, 1], color='orange', linestyle='--', alpha=0.7)\n",
    "axes[0, 1].axhline(true_sigma[0, 1], color='green', linestyle='--', alpha=0.7)\n",
    "axes[0, 1].set_title('Trace: Covariance Parameters')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Joint posterior of mu\n",
    "axes[0, 2].scatter(mu_samples[:, 0], mu_samples[:, 1], alpha=0.5, s=1)\n",
    "axes[0, 2].scatter(true_mu[0], true_mu[1], color='red', s=100, marker='x', \n",
    "                  linewidth=3, label='True μ')\n",
    "axes[0, 2].set_xlabel('μ₁')\n",
    "axes[0, 2].set_ylabel('μ₂')\n",
    "axes[0, 2].set_title('Joint Posterior of μ')\n",
    "axes[0, 2].legend()\n",
    "axes[0, 2].grid(True, alpha=0.3)\n",
    "\n",
    "# Data visualization\n",
    "observed_idx = ~np.isnan(data_observed[:, 1])\n",
    "missing_idx = np.isnan(data_observed[:, 1])\n",
    "\n",
    "axes[1, 0].scatter(data_observed[observed_idx, 0], data_observed[observed_idx, 1], \n",
    "                  alpha=0.7, label='Observed')\n",
    "axes[1, 0].scatter(data_complete[missing_idx, 0], data_complete[missing_idx, 1], \n",
    "                  alpha=0.7, marker='s', label='True missing')\n",
    "\n",
    "# Show imputed values\n",
    "if len(missing_samples) > 0:\n",
    "    imputed_mean = np.mean(missing_samples, axis=0)\n",
    "    axes[1, 0].scatter(data_observed[missing_idx, 0], imputed_mean, \n",
    "                      alpha=0.7, marker='^', label='Imputed')\n",
    "\n",
    "axes[1, 0].set_xlabel('Variable 1')\n",
    "axes[1, 0].set_ylabel('Variable 2')\n",
    "axes[1, 0].set_title('Data and Imputations')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Posterior distributions\n",
    "axes[1, 1].hist(mu_samples[:, 0], bins=30, alpha=0.7, density=True, label='μ₁')\n",
    "axes[1, 1].hist(mu_samples[:, 1], bins=30, alpha=0.7, density=True, label='μ₂')\n",
    "axes[1, 1].axvline(true_mu[0], color='blue', linestyle='--', alpha=0.7)\n",
    "axes[1, 1].axvline(true_mu[1], color='orange', linestyle='--', alpha=0.7)\n",
    "axes[1, 1].set_xlabel('Value')\n",
    "axes[1, 1].set_ylabel('Density')\n",
    "axes[1, 1].set_title('Marginal Posteriors of μ')\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Autocorrelation\n",
    "def autocorrelation(x, max_lag=50):\n",
    "    x_centered = x - np.mean(x)\n",
    "    autocorr = np.correlate(x_centered, x_centered, mode='full')\n",
    "    autocorr = autocorr[len(autocorr)//2:len(autocorr)//2 + max_lag + 1]\n",
    "    return autocorr / autocorr[0]\n",
    "\n",
    "lags = range(51)\n",
    "autocorr_mu1 = autocorrelation(mu_samples[:, 0])\n",
    "autocorr_mu2 = autocorrelation(mu_samples[:, 1])\n",
    "\n",
    "axes[1, 2].plot(lags, autocorr_mu1, 'b-', label='μ₁')\n",
    "axes[1, 2].plot(lags, autocorr_mu2, 'r-', label='μ₂')\n",
    "axes[1, 2].axhline(0, color='black', linestyle='--', alpha=0.5)\n",
    "axes[1, 2].set_xlabel('Lag')\n",
    "axes[1, 2].set_ylabel('Autocorrelation')\n",
    "axes[1, 2].set_title('Autocorrelation Function')\n",
    "axes[1, 2].legend()\n",
    "axes[1, 2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. MCMC Diagnostics\n",
    "\n",
    "### Key Diagnostics:\n",
    "1. **Trace plots**: Visual inspection of mixing\n",
    "2. **R-hat (Gelman-Rubin)**: Convergence diagnostic\n",
    "3. **Effective Sample Size**: Accounting for autocorrelation\n",
    "4. **Autocorrelation**: Chain efficiency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MCMC Diagnostics with PyMC\n",
    "# Example: Logistic regression with potential convergence issues\n",
    "\n",
    "# Generate challenging data (separation issues)\n",
    "np.random.seed(42)\n",
    "n = 100\n",
    "x = np.random.normal(0, 1, n)\n",
    "# Create near-separation\n",
    "prob = 1 / (1 + np.exp(-5 * x))  # Strong effect\n",
    "y = np.random.binomial(1, prob, n)\n",
    "\n",
    "print(f\"Data: {np.sum(y)} successes out of {n} trials\")\n",
    "print(f\"Separation check: min(x[y=0])={np.min(x[y==0]):.2f}, max(x[y=1])={np.max(x[y==1]):.2f}\")\n",
    "\n",
    "# Fit model with different samplers\n",
    "with pm.Model() as logistic_model:\n",
    "    # Priors\n",
    "    alpha = pm.Normal('alpha', 0, 10)  # Wide prior (potential issue)\n",
    "    beta = pm.Normal('beta', 0, 10)\n",
    "    \n",
    "    # Model\n",
    "    logit_p = alpha + beta * x\n",
    "    p = pm.Deterministic('p', pm.math.sigmoid(logit_p))\n",
    "    \n",
    "    # Likelihood\n",
    "    y_obs = pm.Bernoulli('y_obs', p=p, observed=y)\n",
    "    \n",
    "    # Sample with multiple chains\n",
    "    trace_diag = pm.sample(1000, chains=4, return_inferencedata=True, \n",
    "                          random_seed=42, target_accept=0.8)\n",
    "\n",
    "# Comprehensive diagnostics\n",
    "print(\"\\nMCMC Diagnostics:\")\n",
    "print(az.summary(trace_diag, var_names=['alpha', 'beta']))\n",
    "\n",
    "# Visualization of diagnostics\n",
    "fig, axes = plt.subplots(3, 2, figsize=(12, 12))\n",
    "\n",
    "# Trace plots\n",
    "az.plot_trace(trace_diag, var_names=['alpha', 'beta'], axes=axes[:2, :])\n",
    "\n",
    "# R-hat evolution\n",
    "rhat_alpha = []\n",
    "rhat_beta = []\n",
    "sample_sizes = range(100, 1001, 50)\n",
    "\n",
    "for n_samp in sample_sizes:\n",
    "    trace_subset = trace_diag.sel(draw=slice(0, n_samp))\n",
    "    summary = az.summary(trace_subset, var_names=['alpha', 'beta'])\n",
    "    rhat_alpha.append(summary.loc['alpha', 'r_hat'])\n",
    "    rhat_beta.append(summary.loc['beta', 'r_hat'])\n",
    "\n",
    "axes[2, 0].plot(sample_sizes, rhat_alpha, 'b-', label='α')\n",
    "axes[2, 0].plot(sample_sizes, rhat_beta, 'r-', label='β')\n",
    "axes[2, 0].axhline(1.01, color='green', linestyle='--', label='Good (< 1.01)')\n",
    "axes[2, 0].axhline(1.1, color='orange', linestyle='--', label='Acceptable (< 1.1)')\n",
    "axes[2, 0].set_xlabel('Sample Size')\n",
    "axes[2, 0].set_ylabel('R-hat')\n",
    "axes[2, 0].set_title('R-hat Convergence')\n",
    "axes[2, 0].legend()\n",
    "axes[2, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Effective sample size\n",
    "summary = az.summary(trace_diag, var_names=['alpha', 'beta'])\n",
    "ess_bulk = [summary.loc['alpha', 'ess_bulk'], summary.loc['beta', 'ess_bulk']]\n",
    "ess_tail = [summary.loc['alpha', 'ess_tail'], summary.loc['beta', 'ess_tail']]\n",
    "\n",
    "x_pos = [0, 1]\n",
    "width = 0.35\n",
    "\n",
    "axes[2, 1].bar([x - width/2 for x in x_pos], ess_bulk, width, \n",
    "              label='ESS Bulk', alpha=0.7)\n",
    "axes[2, 1].bar([x + width/2 for x in x_pos], ess_tail, width, \n",
    "              label='ESS Tail', alpha=0.7)\n",
    "axes[2, 1].axhline(400, color='green', linestyle='--', label='Good (> 400)')\n",
    "axes[2, 1].axhline(100, color='orange', linestyle='--', label='Minimum (> 100)')\n",
    "axes[2, 1].set_xticks(x_pos)\n",
    "axes[2, 1].set_xticklabels(['α', 'β'])\n",
    "axes[2, 1].set_ylabel('Effective Sample Size')\n",
    "axes[2, 1].set_title('Effective Sample Size')\n",
    "axes[2, 1].legend()\n",
    "axes[2, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Additional diagnostics\n",
    "print(\"\\nDetailed Diagnostics:\")\n",
    "\n",
    "# Energy diagnostics\n",
    "try:\n",
    "    energy_stats = az.bfmi(trace_diag)\n",
    "    print(f\"BFMI (should be > 0.2): {energy_stats}\")\n",
    "except:\n",
    "    print(\"BFMI calculation failed\")\n",
    "\n",
    "# Divergences\n",
    "divergences = trace_diag.sample_stats.diverging.sum().values\n",
    "print(f\"Number of divergences: {divergences}\")\n",
    "\n",
    "# Tree depth\n",
    "max_treedepth = trace_diag.sample_stats.tree_depth.max().values\n",
    "print(f\"Maximum tree depth: {max_treedepth}\")\n",
    "\n",
    "# Acceptance rate\n",
    "accept_rate = trace_diag.sample_stats.accept.mean().values\n",
    "print(f\"Average acceptance rate: {accept_rate:.3f}\")\n",
    "\n",
    "# Plot energy diagnostics\n",
    "if 'energy' in trace_diag.sample_stats:\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(8, 4))\n",
    "    az.plot_energy(trace_diag, ax=ax)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "### MCMC Algorithms:\n",
    "- **Metropolis**: General purpose, requires tuning\n",
    "- **Gibbs**: Efficient when conditionals available\n",
    "- **HMC/NUTS**: Modern, efficient for continuous parameters\n",
    "\n",
    "### Diagnostics:\n",
    "- **R-hat < 1.01**: Good convergence\n",
    "- **ESS > 400**: Adequate for inference\n",
    "- **No divergences**: Sampler exploring properly\n",
    "- **Visual inspection**: Always check trace plots\n",
    "\n",
    "### Best Practices:\n",
    "- **Multiple chains**: Essential for convergence assessment\n",
    "- **Adequate burnin**: Discard initial samples\n",
    "- **Thinning**: May help with autocorrelation\n",
    "- **Reparameterization**: Can improve efficiency\n",
    "\n",
    "## Next: Topic 8 - Applications"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}